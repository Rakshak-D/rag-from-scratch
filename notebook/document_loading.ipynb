{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# This notebook only covers document loading. No chunking, embedding, or vector storage is done here.\n",
   "id": "7c60d462edbf6b6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Document Structure",
   "id": "171df094dfb9bfda"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:15.856676700Z",
     "start_time": "2026-01-13T16:52:15.734884100Z"
    }
   },
   "source": [
    "# Used to create a Document structure mainly consisting page_content and metadata .\n",
    "from langchain_core.documents import Document"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LangChain Document Structure\n",
    "\n",
    "A `Document` has two core fields:\n",
    "- `page_content`: the actual text\n",
    "- `metadata`: contextual information (source, page, file path)\n",
    "\n",
    "This structure is what vector stores consume.\n"
   ],
   "id": "53974b80c834f953"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:15.873181300Z",
     "start_time": "2026-01-13T16:52:15.857682800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Understanding the document structure of langchain Document .\n",
    "doc = Document(\n",
    "    page_content=\"Biography\", # Content of the source .\n",
    "    metadata={\n",
    "        \"Author\": \"Rakshak\",\n",
    "        \"DOB\": \"31-10-2005\",\n",
    "        \"Gender\": \"Male\",\n",
    "        \"Course\": \"AIML\"\n",
    "    } # Details about the content of the source\n",
    ")\n",
    "print(doc) # Display page_content and metadata ."
   ],
   "id": "7d6a322b0518613e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Biography' metadata={'Author': 'Rakshak', 'DOB': '31-10-2005', 'Gender': 'Male', 'Course': 'AIML'}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:15.889691600Z",
     "start_time": "2026-01-13T16:52:15.873181300Z"
    }
   },
   "cell_type": "code",
   "source": "print(doc.page_content) # Can access particular parameter .",
   "id": "9678ae3460c582ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biography\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:15.907207800Z",
     "start_time": "2026-01-13T16:52:15.890698Z"
    }
   },
   "cell_type": "code",
   "source": "print(doc.metadata) # Accessing metadata .",
   "id": "b930b34b36e80a66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Author': 'Rakshak', 'DOB': '31-10-2005', 'Gender': 'Male', 'Course': 'AIML'}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:15.925290600Z",
     "start_time": "2026-01-13T16:52:15.908208800Z"
    }
   },
   "cell_type": "code",
   "source": "print(type(doc)) # Type of the Document .",
   "id": "d3040717964e4bcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Ingestion",
   "id": "419d9292c79f9f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:15.933806500Z",
     "start_time": "2026-01-13T16:52:15.925290600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a directory for a  simple text file\n",
    "import os\n",
    "try:\n",
    "    os.makedirs(\"../data/text\", exist_ok=True) # Just a file location to store test files .\n",
    "except Exception as e:\n",
    "    print(f\" Error creating directory. Error :{e}\") # Exception handling ."
   ],
   "id": "e68af4302ca39a00",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:15.942775800Z",
     "start_time": "2026-01-13T16:52:15.934806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Just some placeholder text for testing . Structure -> {Location of the file : Content }\n",
    "sample = {\n",
    "    \"../data/text/file1.txt\": \"\"\"\n",
    "Abstract\n",
    "\n",
    "Large pre-trained language models have been shown to store factual knowledge in their parameters and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems.\n",
    "\n",
    "Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation.\n",
    "\n",
    "We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations: one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token.\n",
    "\n",
    "We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open-domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.\n",
    "\n",
    "1. Introduction\n",
    "\n",
    "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data. They can do so without any access to an external memory, acting as a parameterized implicit knowledge base. While this development is exciting, such models have downsides: they cannot easily expand or revise their memory, cannot straightforwardly provide insight into their predictions, and may produce hallucinations.\n",
    "\n",
    "Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted.\n",
    "\"\"\",\n",
    "\n",
    "    \"../data/text/file2.txt\": \"\"\"\n",
    "Abstract\n",
    "\n",
    "arXiv:1810.04805v2 [cs.CL] 24 May 2019\n",
    "\n",
    "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\n",
    "\n",
    "As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "\n",
    "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5%, MultiNLI accuracy to 86.7%, SQuAD v1.1 Test F1 to 93.2, and SQuAD v2.0 Test F1 to 83.1.\n",
    "\n",
    "1. Introduction\n",
    "\n",
    "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, as well as token-level tasks such as named entity recognition and question answering.\n",
    "\n",
    "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as GPT, introduces minimal task-specific parameters and is trained on downstream tasks by simply fine-tuning all pre-trained parameters.\n",
    "\n",
    "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n",
    "\"\"\"\n",
    "}"
   ],
   "id": "78493bbe96e76424",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:15.960816800Z",
     "start_time": "2026-01-13T16:52:15.942775800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Accessing the file and writing down the contents .\n",
    "try:\n",
    "    for filename, content in sample.items():\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as f: # UTF-8 ensures consistent text decoding across platforms .\n",
    "            f.write(content)\n",
    "    print(\"Successful File creation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while creating or writing the content . Error : {e}\") # Exception handling ."
   ],
   "id": "dbb0c9cd2091e47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful File creation.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.283256400Z",
     "start_time": "2026-01-13T16:52:15.961823500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A textloader which is used to load texts from files and can also automatically detect the data and structure content,metadata and other essential components .\n",
    "# from langchain.document_loaders import TextLoader #Should have worked but not working for now .\n",
    "from langchain_community.document_loaders import TextLoader #The function used to load data ."
   ],
   "id": "a823c49cb84b924",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.372345300Z",
     "start_time": "2026-01-13T16:52:23.354567400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading the text through file using file location .\n",
    "try:\n",
    "    file_loader = TextLoader(\"../data/text/file1.txt\", encoding=\"utf-8\") # Using utf-8 (Unicode Transformation Format) for encoding .\n",
    "    print(file_loader.load())  # Automatically loads data and gives meta data .\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the text file . Error : {e}\") # Exception handling ."
   ],
   "id": "57d94f8c997b3933",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text/file1.txt'}, page_content='\\nAbstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems.\\n\\nPre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation.\\n\\nWe introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations: one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token.\\n\\nWe fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open-domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n1. Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data. They can do so without any access to an external memory, acting as a parameterized implicit knowledge base. While this development is exciting, such models have downsides: they cannot easily expand or revise their memory, cannot straightforwardly provide insight into their predictions, and may produce hallucinations.\\n\\nHybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted.\\n')]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.389691800Z",
     "start_time": "2026-01-13T16:52:23.373356400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(file_loader.load())) # It is List type so it can be accessed through index .\n",
    "print(file_loader.load()[0]) # Apparently it only can iterate to 0th index since it is only one file ."
   ],
   "id": "e14a4ea1b22390a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "page_content='\n",
      "Abstract\n",
      "\n",
      "Large pre-trained language models have been shown to store factual knowledge in their parameters and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems.\n",
      "\n",
      "Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation.\n",
      "\n",
      "We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations: one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token.\n",
      "\n",
      "We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open-domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data. They can do so without any access to an external memory, acting as a parameterized implicit knowledge base. While this development is exciting, such models have downsides: they cannot easily expand or revise their memory, cannot straightforwardly provide insight into their predictions, and may produce hallucinations.\n",
      "\n",
      "Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted.\n",
      "' metadata={'source': '../data/text/file1.txt'}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.400170100Z",
     "start_time": "2026-01-13T16:52:23.389691800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A DirectoryLoader is used to load entire directory and iterate to access all the files and load data .\n",
    "from langchain_community.document_loaders import DirectoryLoader # The function used to load directory ."
   ],
   "id": "244df1eb0100852d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.407296500Z",
     "start_time": "2026-01-13T16:52:23.401168800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading the text through file accessing by a directory location .\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text\", # Location of directory .\n",
    "    glob=\"**/*.txt\", # File type to search .\n",
    "    loader_cls=TextLoader, # Method using to load data .\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}, # Encoding data .\n",
    "    show_progress=True # A progress bar .(Optional)\n",
    ")"
   ],
   "id": "f678e8cd88611efa",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.439046Z",
     "start_time": "2026-01-13T16:52:23.408293400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    text_doc = dir_loader.load() # Loading the files in the directory .\n",
    "    print(text_doc) # Prints all the files data .\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the text files . Error : {e}\") # Exception handling ."
   ],
   "id": "4631a78988b0e5de",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '..\\\\data\\\\text\\\\file1.txt'}, page_content='\\nAbstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems.\\n\\nPre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation.\\n\\nWe introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations: one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token.\\n\\nWe fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open-domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n1. Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data. They can do so without any access to an external memory, acting as a parameterized implicit knowledge base. While this development is exciting, such models have downsides: they cannot easily expand or revise their memory, cannot straightforwardly provide insight into their predictions, and may produce hallucinations.\\n\\nHybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted.\\n'), Document(metadata={'source': '..\\\\data\\\\text\\\\file2.txt'}, page_content='\\nAbstract\\n\\narXiv:1810.04805v2 [cs.CL] 24 May 2019\\n\\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\\n\\nAs a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\\n\\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5%, MultiNLI accuracy to 86.7%, SQuAD v1.1 Test F1 to 93.2, and SQuAD v2.0 Test F1 to 83.1.\\n\\n1. Introduction\\n\\nLanguage model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, as well as token-level tasks such as named entity recognition and question answering.\\n\\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as GPT, introduces minimal task-specific parameters and is trained on downstream tasks by simply fine-tuning all pre-trained parameters.\\n\\nThe two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loaders always return a list of Documents because:\n",
    "- A single file can produce multiple chunks\n",
    "- A directory produces multiple files\n",
    "- RAG systems always work on collections, not single documents\n"
   ],
   "id": "682f41db928b3acd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.465785900Z",
     "start_time": "2026-01-13T16:52:23.448045100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(text_doc))  # This is a list so a particular file can be accessed using index .\n",
    "print(text_doc[0]) # Accessing file using index ."
   ],
   "id": "9025b80804bb196",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "page_content='\n",
      "Abstract\n",
      "\n",
      "Large pre-trained language models have been shown to store factual knowledge in their parameters and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems.\n",
      "\n",
      "Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation.\n",
      "\n",
      "We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations: one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token.\n",
      "\n",
      "We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open-domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data. They can do so without any access to an external memory, acting as a parameterized implicit knowledge base. While this development is exciting, such models have downsides: they cannot easily expand or revise their memory, cannot straightforwardly provide insight into their predictions, and may produce hallucinations.\n",
      "\n",
      "Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted.\n",
      "' metadata={'source': '..\\\\data\\\\text\\\\file1.txt'}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.531008300Z",
     "start_time": "2026-01-13T16:52:23.465785900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The following functions are used to load pdf documents . PyMuPDFLoader is considered better than PyPDFLoader for extra features .\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader # Function to load pdf files ."
   ],
   "id": "b6b87a95284cc877",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.538638500Z",
     "start_time": "2026-01-13T16:52:23.532315600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading pdf documents in a directory .\n",
    "pdf_loader = DirectoryLoader(\n",
    "    path=\"../data\", # Directory location .\n",
    "    glob=\"**/*.pdf\", # File type used to search particular file type .\n",
    "    loader_cls=PyMuPDFLoader, # Method to load the files .\n",
    "    # loader_kwargs={'encoding':\"utf-8\"}, # Encoding parameter cannot be used here .\n",
    "    show_progress=True # Used to show progress .(Optional)\n",
    ")"
   ],
   "id": "b5767469d26ed507",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.865825Z",
     "start_time": "2026-01-13T16:52:23.538638500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    pdf_doc=pdf_loader.load() # Loading the files in directory .\n",
    "    print(pdf_doc[18]) # Loading only one pdf due to large number of data .\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the pdf files . Error : {e}\") # Exception handling ."
   ],
   "id": "ab32557da1d36b3f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Document 1: his works are considered classics of American\n",
      "literature ... His wartime experiences formed the basis for his novel\n",
      "”A Farewell to Arms” (1929) ...\n",
      "Document 2: ... artists of the 1920s ”Lost Generation” expatriate\n",
      "community. His debut novel, ”The Sun Also Rises”, was published\n",
      "in 1926.\n",
      "BOS ”\n",
      "The\n",
      "Sun\n",
      "Also\n",
      "R\n",
      "ises\n",
      "”\n",
      "is\n",
      "a\n",
      "novel\n",
      "by\n",
      "this\n",
      "authorof\n",
      "”\n",
      "A\n",
      "Fare\n",
      "well\n",
      "to\n",
      "Arms”\n",
      "Doc 1\n",
      "Doc 2\n",
      "Doc 3\n",
      "Doc 4\n",
      "Doc 5\n",
      "Figure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hem-\n",
      "ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\n",
      "when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\n",
      "Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\n",
      "responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\n",
      "Task\n",
      "Input\n",
      "Model\n",
      "Generation\n",
      "MS-\n",
      "MARCO\n",
      "deﬁne middle\n",
      "ear\n",
      "BART\n",
      "?The middle ear is the part of the ear between the middle ear and the nose.\n",
      "RAG-T The middle ear is the portion of the ear internal to the eardrum.\n",
      "RAG-S The middle ear includes the tympanic cavity and the three ossicles.\n",
      "what currency\n",
      "needed in\n",
      "scotland\n",
      "BART\n",
      "The currency needed in Scotland is Pound sterling.\n",
      "RAG-T Pound is the currency needed in Scotland.\n",
      "RAG-S The currency needed in Scotland is the pound sterling.\n",
      "Jeopardy\n",
      "Question\n",
      "Gener\n",
      "-ation\n",
      "Washington\n",
      "BART\n",
      "?This state has the largest number of counties in the U.S.\n",
      "RAG-T It’s the only U.S. state named for a U.S. president\n",
      "RAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park\n",
      "The Divine\n",
      "Comedy\n",
      "BART\n",
      "*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\n",
      "RAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem\n",
      "RAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\n",
      "For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\n",
      "to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\n",
      "within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\n",
      "We also analyze whether documents retrieved by RAG correspond to documents annotated as gold\n",
      "evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved\n",
      "by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\n",
      "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
      "4.5\n",
      "Additional Results\n",
      "Generation Diversity\n",
      "Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
      "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
      "[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
      "total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\n",
      "more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\n",
      "any diversity-promoting decoding.\n",
      "Retrieval Ablations\n",
      "A key feature of RAG is learning to retrieve relevant information for the task.\n",
      "To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\n",
      "during training. As shown in Table 6, learned retrieval improves results for all tasks.\n",
      "We compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace\n",
      "RAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating\n",
      "p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\n",
      "heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\n",
      "improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\n",
      "Index hot-swapping\n",
      "An advantage of non-parametric memory models like RAG is that knowledge\n",
      "can be easily updated at test time. Parametric-only models like T5 or BART need further training to\n",
      "update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]\n",
      "Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\n",
      "index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\n",
      "7' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.884441400Z",
     "start_time": "2026-01-13T16:52:23.865825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(pdf_doc)) # It is a list type so files can be accessed through index .\n",
    "print(pdf_doc[0]) # Accessing files through index ."
   ],
   "id": "7b64730e0ab14358",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "page_content='Deep Residual Learning for Image Recognition\n",
      "Kaiming He\n",
      "Xiangyu Zhang\n",
      "Shaoqing Ren\n",
      "Jian Sun\n",
      "Microsoft Research\n",
      "{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\n",
      "Abstract\n",
      "Deeper neural networks are more difﬁcult to train. We\n",
      "present a residual learning framework to ease the training\n",
      "of networks that are substantially deeper than those used\n",
      "previously. We explicitly reformulate the layers as learn-\n",
      "ing residual functions with reference to the layer inputs, in-\n",
      "stead of learning unreferenced functions. We provide com-\n",
      "prehensive empirical evidence showing that these residual\n",
      "networks are easier to optimize, and can gain accuracy from\n",
      "considerably increased depth. On the ImageNet dataset we\n",
      "evaluate residual nets with a depth of up to 152 layers—8×\n",
      "deeper than VGG nets [41] but still having lower complex-\n",
      "ity. An ensemble of these residual nets achieves 3.57% error\n",
      "on the ImageNet test set. This result won the 1st place on the\n",
      "ILSVRC 2015 classiﬁcation task. We also present analysis\n",
      "on CIFAR-10 with 100 and 1000 layers.\n",
      "The depth of representations is of central importance\n",
      "for many visual recognition tasks. Solely due to our ex-\n",
      "tremely deep representations, we obtain a 28% relative im-\n",
      "provement on the COCO object detection dataset. Deep\n",
      "residual nets are foundations of our submissions to ILSVRC\n",
      "& COCO 2015 competitions1, where we also won the 1st\n",
      "places on the tasks of ImageNet detection, ImageNet local-\n",
      "ization, COCO detection, and COCO segmentation.\n",
      "1. Introduction\n",
      "Deep convolutional neural networks [22, 21] have led\n",
      "to a series of breakthroughs for image classiﬁcation [21,\n",
      "50, 40]. Deep networks naturally integrate low/mid/high-\n",
      "level features [50] and classiﬁers in an end-to-end multi-\n",
      "layer fashion, and the “levels” of features can be enriched\n",
      "by the number of stacked layers (depth). Recent evidence\n",
      "[41, 44] reveals that network depth is of crucial importance,\n",
      "and the leading results [41, 44, 13, 16] on the challenging\n",
      "ImageNet dataset [36] all exploit “very deep” [41] models,\n",
      "with a depth of sixteen [41] to thirty [16]. Many other non-\n",
      "trivial visual recognition tasks [8, 12, 7, 32, 27] have also\n",
      "1http://image-net.org/challenges/LSVRC/2015/\n",
      "and\n",
      "http://mscoco.org/dataset/#detections-challenge2015.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0 \n",
      "10\n",
      "20\n",
      "iter. (1e4)\n",
      "training error (%)\n",
      " \n",
      " \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "10\n",
      "20\n",
      "iter. (1e4)\n",
      "test error (%)\n",
      " \n",
      " \n",
      "56-layer\n",
      "20-layer\n",
      "56-layer\n",
      "20-layer\n",
      "Figure 1. Training error (left) and test error (right) on CIFAR-10\n",
      "with 20-layer and 56-layer “plain” networks. The deeper network\n",
      "has higher training error, and thus test error. Similar phenomena\n",
      "on ImageNet is presented in Fig. 4.\n",
      "greatly beneﬁted from very deep models.\n",
      "Driven by the signiﬁcance of depth, a question arises: Is\n",
      "learning better networks as easy as stacking more layers?\n",
      "An obstacle to answering this question was the notorious\n",
      "problem of vanishing/exploding gradients [1, 9], which\n",
      "hamper convergence from the beginning.\n",
      "This problem,\n",
      "however, has been largely addressed by normalized initial-\n",
      "ization [23, 9, 37, 13] and intermediate normalization layers\n",
      "[16], which enable networks with tens of layers to start con-\n",
      "verging for stochastic gradient descent (SGD) with back-\n",
      "propagation [22].\n",
      "When deeper networks are able to start converging, a\n",
      "degradation problem has been exposed: with the network\n",
      "depth increasing, accuracy gets saturated (which might be\n",
      "unsurprising) and then degrades rapidly.\n",
      "Unexpectedly,\n",
      "such degradation is not caused by overﬁtting, and adding\n",
      "more layers to a suitably deep model leads to higher train-\n",
      "ing error, as reported in [11, 42] and thoroughly veriﬁed by\n",
      "our experiments. Fig. 1 shows a typical example.\n",
      "The degradation (of training accuracy) indicates that not\n",
      "all systems are similarly easy to optimize. Let us consider a\n",
      "shallower architecture and its deeper counterpart that adds\n",
      "more layers onto it. There exists a solution by construction\n",
      "to the deeper model: the added layers are identity mapping,\n",
      "and the other layers are copied from the learned shallower\n",
      "model. The existence of this constructed solution indicates\n",
      "that a deeper model should produce no higher training error\n",
      "than its shallower counterpart. But experiments show that\n",
      "our current solvers on hand are unable to ﬁnd solutions that\n",
      "1\n",
      "arXiv:1512.03385v1  [cs.CV]  10 Dec 2015' metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '..\\\\data\\\\pdf\\\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf', 'file_path': '..\\\\data\\\\pdf\\\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 0}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Typical metadata includes:\n",
    "- `source`: file path\n",
    "- file name\n",
    "- directory\n"
   ],
   "id": "1af563456605b200"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:23.902648200Z",
     "start_time": "2026-01-13T16:52:23.886440600Z"
    }
   },
   "cell_type": "code",
   "source": "print(pdf_doc[0].metadata) # Meta data of a file . Generated automatically .",
   "id": "f2061d725e5bd4df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '..\\\\data\\\\pdf\\\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf', 'file_path': '..\\\\data\\\\pdf\\\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 0}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:24.016553400Z",
     "start_time": "2026-01-13T16:52:23.903649600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# WebBaseLoader is used to load data from websites and it used beautifusoup4 for web scarping .\n",
    "from langchain_community.document_loaders import WebBaseLoader # The function to load data ."
   ],
   "id": "4c3526b5e8c48450",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:24.025036800Z",
     "start_time": "2026-01-13T16:52:24.017554600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading a website data using the website link .\n",
    "link = WebBaseLoader(\n",
    "    web_path=\"https://httpbin.org/html\",# Link of the test website .\n",
    "    encoding=\"utf-8\" # Encoding method .\n",
    ")"
   ],
   "id": "7a079996ba7b3fa5",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:52:26.888954500Z",
     "start_time": "2026-01-13T16:52:24.026035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    web_loader=link.load() # Loading the data .\n",
    "    print(web_loader)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the web data . Error : {e}\") # Exception handling ."
   ],
   "id": "e7470f20b7d262e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://httpbin.org/html', 'language': 'No language found.'}, page_content=\"\\n\\n\\n\\n\\nHerman Melville - Moby-Dick\\n\\n\\n          Availing himself of the mild, summer-cool weather that now reigned in these latitudes, and in preparation for the peculiarly active pursuits shortly to be anticipated, Perth, the begrimed, blistered old blacksmith, had not removed his portable forge to the hold again, after concluding his contributory work for Ahab's leg, but still retained it on deck, fast lashed to ringbolts by the foremast; being now almost incessantly invoked by the headsmen, and harpooneers, and bowsmen to do some little job for them; altering, or repairing, or new shaping their various weapons and boat furniture. Often he would be surrounded by an eager circle, all waiting to be served; holding boat-spades, pike-heads, harpoons, and lances, and jealously watching his every sooty movement, as he toiled. Nevertheless, this old man's was a patient hammer wielded by a patient arm. No murmur, no impatience, no petulance did come from him. Silent, slow, and solemn; bowing over still further his chronically broken back, he toiled away, as if toil were life itself, and the heavy beating of his hammer the heavy beating of his heart. And so it was.—Most miserable! A peculiar walk in this old man, a certain slight but painful appearing yawing in his gait, had at an early period of the voyage excited the curiosity of the mariners. And to the importunity of their persisted questionings he had finally given in; and so it came to pass that every one now knew the shameful story of his wretched fate. Belated, and not innocently, one bitter winter's midnight, on the road running between two country towns, the blacksmith half-stupidly felt the deadly numbness stealing over him, and sought refuge in a leaning, dilapidated barn. The issue was, the loss of the extremities of both feet. Out of this revelation, part by part, at last came out the four acts of the gladness, and the one long, and as yet uncatastrophied fifth act of the grief of his life's drama. He was an old man, who, at the age of nearly sixty, had postponedly encountered that thing in sorrow's technicals called ruin. He had been an artisan of famed excellence, and with plenty to do; owned a house and garden; embraced a youthful, daughter-like, loving wife, and three blithe, ruddy children; every Sunday went to a cheerful-looking church, planted in a grove. But one night, under cover of darkness, and further concealed in a most cunning disguisement, a desperate burglar slid into his happy home, and robbed them all of everything. And darker yet to tell, the blacksmith himself did ignorantly conduct this burglar into his family's heart. It was the Bottle Conjuror! Upon the opening of that fatal cork, forth flew the fiend, and shrivelled up his home. Now, for prudent, most wise, and economic reasons, the blacksmith's shop was in the basement of his dwelling, but with a separate entrance to it; so that always had the young and loving healthy wife listened with no unhappy nervousness, but with vigorous pleasure, to the stout ringing of her young-armed old husband's hammer; whose reverberations, muffled by passing through the floors and walls, came up to her, not unsweetly, in her nursery; and so, to stout Labor's iron lullaby, the blacksmith's infants were rocked to slumber. Oh, woe on woe! Oh, Death, why canst thou not sometimes be timely? Hadst thou taken this old blacksmith to thyself ere his full ruin came upon him, then had the young widow had a delicious grief, and her orphans a truly venerable, legendary sire to dream of in their after years; and all of them a care-killing competency.\\n        \\n\\n\\n\")]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-13T16:52:26.902837900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(web_loader)) # It is a List type so it can be accessed through index .\n",
    "print(web_loader[0]) # Apparently it only can iterate to 0th index since it is only one web link ."
   ],
   "id": "63c4056282dabd49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "page_content='\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Herman Melville - Moby-Dick\n",
      "\n",
      "\n",
      "          Availing himself of the mild, summer-cool weather that now reigned in these latitudes, and in preparation for the peculiarly active pursuits shortly to be anticipated, Perth, the begrimed, blistered old blacksmith, had not removed his portable forge to the hold again, after concluding his contributory work for Ahab's leg, but still retained it on deck, fast lashed to ringbolts by the foremast; being now almost incessantly invoked by the headsmen, and harpooneers, and bowsmen to do some little job for them; altering, or repairing, or new shaping their various weapons and boat furniture. Often he would be surrounded by an eager circle, all waiting to be served; holding boat-spades, pike-heads, harpoons, and lances, and jealously watching his every sooty movement, as he toiled. Nevertheless, this old man's was a patient hammer wielded by a patient arm. No murmur, no impatience, no petulance did come from him. Silent, slow, and solemn; bowing over still further his chronically broken back, he toiled away, as if toil were life itself, and the heavy beating of his hammer the heavy beating of his heart. And so it was.—Most miserable! A peculiar walk in this old man, a certain slight but painful appearing yawing in his gait, had at an early period of the voyage excited the curiosity of the mariners. And to the importunity of their persisted questionings he had finally given in; and so it came to pass that every one now knew the shameful story of his wretched fate. Belated, and not innocently, one bitter winter's midnight, on the road running between two country towns, the blacksmith half-stupidly felt the deadly numbness stealing over him, and sought refuge in a leaning, dilapidated barn. The issue was, the loss of the extremities of both feet. Out of this revelation, part by part, at last came out the four acts of the gladness, and the one long, and as yet uncatastrophied fifth act of the grief of his life's drama. He was an old man, who, at the age of nearly sixty, had postponedly encountered that thing in sorrow's technicals called ruin. He had been an artisan of famed excellence, and with plenty to do; owned a house and garden; embraced a youthful, daughter-like, loving wife, and three blithe, ruddy children; every Sunday went to a cheerful-looking church, planted in a grove. But one night, under cover of darkness, and further concealed in a most cunning disguisement, a desperate burglar slid into his happy home, and robbed them all of everything. And darker yet to tell, the blacksmith himself did ignorantly conduct this burglar into his family's heart. It was the Bottle Conjuror! Upon the opening of that fatal cork, forth flew the fiend, and shrivelled up his home. Now, for prudent, most wise, and economic reasons, the blacksmith's shop was in the basement of his dwelling, but with a separate entrance to it; so that always had the young and loving healthy wife listened with no unhappy nervousness, but with vigorous pleasure, to the stout ringing of her young-armed old husband's hammer; whose reverberations, muffled by passing through the floors and walls, came up to her, not unsweetly, in her nursery; and so, to stout Labor's iron lullaby, the blacksmith's infants were rocked to slumber. Oh, woe on woe! Oh, Death, why canst thou not sometimes be timely? Hadst thou taken this old blacksmith to thyself ere his full ruin came upon him, then had the young widow had a delicious grief, and her orphans a truly venerable, legendary sire to dream of in their after years; and all of them a care-killing competency.\n",
      "        \n",
      "\n",
      "\n",
      "' metadata={'source': 'https://httpbin.org/html', 'language': 'No language found.'}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Important Note: Documents are NOT embeddings\n",
    "\n",
    "Before vector storage:\n",
    "1. Documents are split into chunks\n",
    "2. Chunks are embedded\n",
    "3. Embeddings are stored\n",
    "\n",
    "This notebook stops at step 1.\n"
   ],
   "id": "b8d39c1563094b94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### What comes next\n",
    "- Text splitting (RecursiveCharacterTextSplitter)\n",
    "- Embedding generation\n",
    "- Vector store ingestion\n",
    "- Retrieval (RAG)\n"
   ],
   "id": "6f0750aec4d1ac45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Next step: splitting documents into chunks before embeddings.\n",
   "id": "ed70b307cc4c5301"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
